{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "kuWmV-FQP_Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvQEB--IjlhU",
        "outputId": "abb27e66-7326-4a2d-91d3-66931843f114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements"
      ],
      "metadata": {
        "id": "cCN_MNX3T5Fl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J-YU9c6Lfxi",
        "outputId": "89c84a3a-9b74-4ee1-8463-907b760bce53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio torch torchvision scikit-learn pandas matplotlib openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "3fyhWBBpNONV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import csv\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Column names\n",
        "colNames = ['coltime', 'gsr', 'ecg', 'emg_trapezius', 'emg_corrugator', 'emg_zygomaticus']\n",
        "\n",
        "# Label dicts\n",
        "pain_labels = {'BL1': 0,'PA1': 1,'PA2': 2,'PA3': 3,'PA4': 4}\n",
        "\n",
        "def process_bioVid(data_dir, output_dir):\n",
        "    \"\"\"\n",
        "    :param output_dir: processed data directory\n",
        "    :param data_dir: data directory\n",
        "    :return: Save BioVid csv files to npz\n",
        "    \"\"\"\n",
        "    # data_dir_files contain 87 subjects' pain signals,\n",
        "    # Each subject has 5*20 (pain intensity level: 5; replicates: 20) samples\n",
        "    data_dir_files = os.listdir(data_dir)\n",
        "    i = 0\n",
        "    # Dimension of files: 5*20\n",
        "    for files in data_dir_files:\n",
        "        parent_path = os.path.join(data_dir, files)\n",
        "        path = glob.glob(parent_path + '/*.csv')\n",
        "\n",
        "        labels_per = []\n",
        "        EDA_signals_per = []\n",
        "        ECG_signals_per = []\n",
        "        EMG_signals_per = []\n",
        "        # One single csv file per iteration included one pain intensity level and its signals\n",
        "        for csv_file in path:\n",
        "            key = csv_file.split('-')[-2]\n",
        "            label = pain_labels[key]\n",
        "            EDA_signal = []\n",
        "            ECG_signal = []\n",
        "            EMG_signal = []\n",
        "            with open(csv_file, newline = '') as f:\n",
        "                next(f)\n",
        "                reader = csv.reader(f, delimiter = '\\t')\n",
        "                for row in reader:\n",
        "                    values = [float(num) for num in row]\n",
        "                    EDA_signal.append(values[1])\n",
        "                    ECG_signal.append(values[2])\n",
        "                    EMG_signal.append(values[3])\n",
        "            EDA_signals_per.append(EDA_signal)\n",
        "            ECG_signals_per.append(ECG_signal)\n",
        "            EMG_signals_per.append(EMG_signal)\n",
        "            labels_per.append(label)\n",
        "        x_eda = np.asarray(EDA_signals_per)\n",
        "        x_ecg = np.asarray(ECG_signals_per)\n",
        "        x_emg = np.asarray(EMG_signals_per)\n",
        "        y = np.asarray(labels_per)\n",
        "        print('Data Processed: {}/{}'.format(i+1, len(data_dir_files)))\n",
        "        print('X_eda, X_ecg, X_emg, y shape:{},{},{}, {}'.format(x_eda.shape,x_ecg.shape,x_emg.shape, y.shape))\n",
        "\n",
        "        filename = os.path.join(output_dir, data_dir_files[i] + '.npz')\n",
        "        signals_dict = {\"x_eda\": x_eda,\"x_ecg\": x_ecg,\"x_emg\": x_emg,\"y\": y}\n",
        "        np.savez(filename, **signals_dict)\n",
        "        file_stats = os.stat(filename)\n",
        "        print(f'File Size - {file_stats.st_size / (1024 * 1024)} MB')\n",
        "        i += 1\n",
        "\n",
        "    print(\"\\n===================Done===================\\n\")\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     input_dir = '/content/drive/MyDrive/BM5020/PartA/biosignals_filtered'\n",
        "#     output_dir = 'processed_bioVid_partA'\n",
        "#     if not os.path.exists(output_dir):\n",
        "#         os.makedirs(output_dir)\n",
        "#     else:\n",
        "#         shutil.rmtree(output_dir)\n",
        "#         os.makedirs(output_dir)\n",
        "#     process_bioVid(input_dir,output_dir)"
      ],
      "metadata": {
        "id": "7JfUXbecNQ3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "s1BoOgNTTzVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Utils.py\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from itertools import repeat\n",
        "import pandas as pd\n",
        "\n",
        "# Label dicts\n",
        "pain_labels = {\n",
        "    'BL1': 0,\n",
        "    'PA1': 1,\n",
        "    'PA2': 2,\n",
        "    'PA3': 3,\n",
        "    'PA4': 4\n",
        "}\n",
        "\n",
        "# Multiclass type\n",
        "class_types_dict = {\n",
        "    '0 vs 1 vs 2 vs 3 vs 4': 0,\n",
        "    '0 vs 4': 1,\n",
        "    '0 vs 1, 2, 3, 4': 2,\n",
        "    '0, 1 vs 3, 4': 3,\n",
        "    '0 vs 3, 4': 4,\n",
        "    '0, 1 vs 4': 5,\n",
        "    '0, 1 vs 3': 6,\n",
        "    '0 vs 1, 2 vs 3, 4': 7\n",
        "}\n",
        "\n",
        "\n",
        "def generate_kfolds_index(npz_dir, k_folds) -> dict[int: list[str]]:\n",
        "    \"\"\"\n",
        "    Generate k-folds dataset index and store into a dictionary. The length of dictionary is equal to the number of\n",
        "    folds. Each element contains training set and testing set.\n",
        "    :param npz_dir: npz files directory\n",
        "    :param k_folds: the number of folds\n",
        "    :return: a dict contains k-folds dataset paths, e.g. dict{0: [list[str(train_dir)], list[str(test_dir)]]..., k:[...]}\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(npz_dir):\n",
        "        print('================= Creating KFolds Index =================')\n",
        "    else:\n",
        "        print('================= Data directory does not exist =================')\n",
        "\n",
        "    npz_files = glob.glob(os.path.join(npz_dir, '*.npz'))\n",
        "    npz_files = np.asarray(npz_files)\n",
        "    kfolds_names = np.array_split(npz_files, k_folds)\n",
        "    # print(kfolds_names)\n",
        "    kfolds_index = {}\n",
        "    for fold_index in range(0, k_folds):\n",
        "        test_data = kfolds_names[fold_index].tolist()\n",
        "        train_data = [files for i, files in enumerate(kfolds_names) if i != fold_index]\n",
        "        train_data = [files for subfiles in train_data for files in subfiles]\n",
        "        kfolds_index[fold_index] = [train_data, test_data]\n",
        "    print('================= {} folds dataset created ================='.format(k_folds))\n",
        "    return kfolds_index\n",
        "\n",
        "\n",
        "class BioVidLoader(Dataset):\n",
        "    \"\"\"\n",
        "    Input: a list of npz files' directories from k-folds index\n",
        "    Output: a tensor of values and labels\n",
        "    \"\"\"\n",
        "    def __init__(self, npz_files, label_converter, signal_types):\n",
        "        super(BioVidLoader, self).__init__()\n",
        "\n",
        "        if len(signal_types) == 1:\n",
        "          # Load first npz file which is easy to handle for the rest\n",
        "          x_values = np.load(npz_files[0])[f'x_{signal_types[0]}']\n",
        "\n",
        "          # Load npz files starting from position 1\n",
        "          for file in npz_files[1:]:\n",
        "            x_values = np.vstack((x_values, np.load(file)[f'x_{signal_types[0]}']))\n",
        "        else:\n",
        "          # Load first npz file which is easy to handle for the rest\n",
        "          x_values = np.load(npz_files[0])[f'x_{signal_types[0]}']\n",
        "          for signal in signal_types[1:]:\n",
        "            x_values = np.hstack((x_values, np.load(npz_files[0])[f'x_{signal}']))\n",
        "\n",
        "          # Load npz files starting from position 1\n",
        "          for file in npz_files[1:]:\n",
        "            x_values_new = np.load(file)[f'x_{signal_types[0]}']\n",
        "            for signal in signal_types[1:]:\n",
        "              x_values_new = np.hstack((x_values_new, np.load(file)[f'x_{signal}']))\n",
        "            x_values = np.vstack((x_values, x_values_new))\n",
        "\n",
        "\n",
        "        # Load first npz file which is easy to handle for the rest\n",
        "        y_labels = np.load(npz_files[0])['y']\n",
        "        # Load npz files starting from position 1\n",
        "        for file in npz_files[1:]:\n",
        "          y_labels = np.append(y_labels, np.load(file)['y'])\n",
        "\n",
        "        # Convert the original labels to the task labels, e.g. 4 --> 1\n",
        "        y_labels = np.array([label_converter[str(label)] for label in y_labels])\n",
        "        # Create masking indices, -1 as False\n",
        "        mask = (y_labels != -1)\n",
        "\n",
        "        # Remove all the values with False\n",
        "        x_values = x_values[mask]\n",
        "        y_labels = y_labels[mask]\n",
        "\n",
        "        self.val = torch.from_numpy(x_values).float()\n",
        "        self.lbl = torch.from_numpy(y_labels).long()\n",
        "\n",
        "        # Change shape to (Batch size, Channel size, Length)\n",
        "        self.val = self.val.unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.val.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.val[idx], self.lbl[idx]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}'.format(repr(self.val))\n",
        "\n",
        "    def __str__(self):\n",
        "        # BioVid: torch.Size([3440, 1, 2816]), torch.Size([3440])\n",
        "        return 'The shape of values and labels: {}, {}'.format(self.val.shape, self.lbl.shape)\n",
        "\n",
        "\n",
        "def load_data(train_set, valid_set,label_converter,batch_size,signal_types, num_workers = 0) -> tuple[DataLoader, DataLoader, list[int]]:\n",
        "    \"\"\"\n",
        "    generate dataloader for both training dataset and validation dataset from one of the k-folds.\n",
        "    :param train_set: training dataset\n",
        "    :param valid_set: validation dataset\n",
        "    :param label_converter: convert the original labels to the desired labels\n",
        "    :param batch_size: batch size\n",
        "    :param num_workers: 4*GPU\n",
        "    :return: dataloader for training dataset, validation dataset, the number of samples for each class,\n",
        "        e.g. two classes -> list[int,int]\n",
        "    \"\"\"\n",
        "    train_dataset = BioVidLoader(train_set, label_converter, signal_types)\n",
        "    valid_dataset = BioVidLoader(valid_set, label_converter, signal_types)\n",
        "\n",
        "    cat_y = torch.cat((train_dataset.lbl, valid_dataset.lbl))\n",
        "\n",
        "    # dist = [cat_y.count(i) for i in range(n_classes)]\n",
        "\n",
        "    # e.g. two classes (tensor(list[lbl, lbl]), tensor(list[int, int]))\n",
        "    unique_counts = cat_y.unique(return_counts = True)\n",
        "    # number of samples for each class -> list[int, int]\n",
        "    dist = unique_counts[1].tolist()\n",
        "\n",
        "    # n_classes = len(unique_counts[0])\n",
        "\n",
        "    # percent = [pop / sum(dist) for pop in dist]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              num_workers = num_workers,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle = True,\n",
        "                              drop_last = False,\n",
        "                              pin_memory = True)\n",
        "\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              num_workers = num_workers,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle = False,\n",
        "                              drop_last = False,\n",
        "                              pin_memory = True)\n",
        "    return train_loader, valid_loader, dist"
      ],
      "metadata": {
        "id": "inwd54wTNFb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "VYbnjDZKTgVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "metrics_manager.py\n",
        "\n",
        "This module contains the implementation of the metrics manager, and other metrics related\n",
        "functions.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, cohen_kappa_score, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        assert pred.shape[0] == len(target)\n",
        "        correct = 0\n",
        "        correct += torch.sum((pred == target).int()).item()\n",
        "    return correct / len(target)\n",
        "\n",
        "\n",
        "def f1(output, target):\n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        assert pred.shape[0] == len(target)\n",
        "    return f1_score(pred.cpu().numpy(), target.data.cpu().numpy(), average='macro')\n",
        "\n",
        "\n",
        "def _calc_metrics(checkpoint_dir,name, fold_id=None):\n",
        "\n",
        "    n_folds = 87\n",
        "    all_outs = []\n",
        "    all_trgs = []\n",
        "\n",
        "    outs_list = []\n",
        "    trgs_list = []\n",
        "    save_dir = os.path.abspath(os.path.join(checkpoint_dir, os.pardir))\n",
        "    for root, dirs, files in os.walk(save_dir):\n",
        "        for file in files:\n",
        "            if \"outs\" in file:\n",
        "                outs_list.append(os.path.join(root, file))\n",
        "            if \"trgs\" in file:\n",
        "                trgs_list.append(os.path.join(root, file))\n",
        "\n",
        "    if fold_id is not None:\n",
        "        outs = np.load(outs_list[fold_id])\n",
        "        trgs = np.load(trgs_list[fold_id])\n",
        "        all_outs.extend(outs)\n",
        "        all_trgs.extend(trgs)\n",
        "        save_dir = os.path.abspath(os.path.join(checkpoint_dir))\n",
        "    elif len(outs_list) == n_folds:\n",
        "        for i in range(len(outs_list)):\n",
        "            outs = np.load(outs_list[i])\n",
        "            trgs = np.load(trgs_list[i])\n",
        "            all_outs.extend(outs)\n",
        "            all_trgs.extend(trgs)\n",
        "\n",
        "    all_trgs = np.array(all_trgs).astype(int)\n",
        "    all_outs = np.array(all_outs).astype(int)\n",
        "\n",
        "    r = classification_report(all_trgs, all_outs, digits = 6, output_dict = True)\n",
        "    cm = confusion_matrix(all_trgs, all_outs)\n",
        "    df = pd.DataFrame(r)\n",
        "    df[\"cohen\"] = cohen_kappa_score(all_trgs, all_outs)\n",
        "    df[\"accuracy\"] = accuracy_score(all_trgs, all_outs)\n",
        "    df = df * 100\n",
        "    if fold_id is not None:\n",
        "      file_name = name + f\"_classification_report_fold{fold_id}.xlsx\"\n",
        "    else:\n",
        "      file_name = name + f\"_classification_report_all_fold.xlsx\"\n",
        "    report_save_path = os.path.join(save_dir, file_name)\n",
        "    df.to_excel(report_save_path)\n",
        "\n",
        "    if fold_id is not None:\n",
        "      cm_file_name = name + f\"_confusion_matrix_fold{fold_id}.torch\"\n",
        "    else:\n",
        "      cm_file_name = name + f\"_confusion_matrix_all_fold.torch\"\n",
        "    cm_Save_path = os.path.join(save_dir, cm_file_name)\n",
        "    torch.save(cm, cm_Save_path)\n",
        "\n",
        "\n",
        "class MetricTracker:\n",
        "    def __init__(self, *keys, writer=None):\n",
        "        self.writer = writer\n",
        "        self._data = pd.DataFrame(index=keys, columns=['total', 'counts', 'average'])\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        for col in self._data.columns:\n",
        "            self._data[col].values[:] = 0\n",
        "\n",
        "    def update(self, key, value, n=1):\n",
        "        if self.writer is not None:\n",
        "            self.writer.add_scalar(key, value)\n",
        "        self._data.total[key] += value * n\n",
        "        self._data.counts[key] += n\n",
        "        self._data.average[key] = self._data.total[key] / self._data.counts[key]\n",
        "\n",
        "    def avg(self, key):\n",
        "        return self._data.average[key]\n",
        "\n",
        "    def result(self):\n",
        "        return dict(self._data.average)"
      ],
      "metadata": {
        "id": "-mnV3GI6NwMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Trainer"
      ],
      "metadata": {
        "id": "Nwp5pZcsTkKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Main_trainer.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from numpy import inf\n",
        "\n",
        "selected_d = {\"outs\": [], \"trg\": []}\n",
        "\n",
        "def _save_checkpoint(model, optimizer, epoch, mnt_best, checkpoint_dir,fold_id, save_best = True):\n",
        "    \"\"\"\n",
        "    Saving checkpoints\n",
        "\n",
        "    :param model: model to be saved\n",
        "    :param optimizer: optimizer to be saved\n",
        "    :param epoch: current epoch number\n",
        "    :param mnt_best: metric used to monitor the best model\n",
        "    :param checkpoint_dir: directory where the checkpoint will be saved\n",
        "    :param save_best: if True, rename the saved checkpoint to 'model_best.pth'\n",
        "    \"\"\"\n",
        "    arch = type(model).__name__\n",
        "    state = {\n",
        "        'arch': arch,\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'monitor_best': mnt_best\n",
        "    }\n",
        "    filename = str('/content/saved/checkpoint-epoch{}-fold{}.pth'.format(epoch,fold_id))\n",
        "    torch.save(state, filename)\n",
        "    print(\"Saving checkpoint: {} ...\".format(filename))\n",
        "    if save_best:\n",
        "        best_path = str(f'/content/saved/model_best_fold{fold_id}.pth')\n",
        "        torch.save(state, best_path)\n",
        "        print(\"Saving current best: model_best.pth ...\")\n",
        "\n",
        "def _prepare_device(n_gpu_use):\n",
        "    \"\"\"\n",
        "    Setup GPU\n",
        "    \"\"\"\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    # n_gpu_use = min(n_gpu, n_gpu_use)\n",
        "    if n_gpu_use > 0 and n_gpu == 0:\n",
        "        print(\"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n",
        "        n_gpu_use = 0\n",
        "    if n_gpu_use > n_gpu:\n",
        "        print(\"Warning: The number of GPU\\'s configured to use is {}, but only {} are available on this machine.\".format(n_gpu_use, n_gpu))\n",
        "        n_gpu_use = n_gpu\n",
        "    device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
        "    list_ids = list(range(n_gpu_use))\n",
        "    return device, list_ids\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss, optimizer, data_loader, fold_id, exp_name,valid_data_loader=None):\n",
        "\n",
        "        # Basic configuration\n",
        "        self.fold_id = fold_id\n",
        "        self.device, device_ids = _prepare_device(1)\n",
        "        self.exp_name = exp_name\n",
        "\n",
        "        # Save checkpoint directory\n",
        "        self.checkpoint_dir = '/content/saved'\n",
        "\n",
        "        # Prepare model\n",
        "        self.model = model.to(self.device)\n",
        "        if len(device_ids) > 1:\n",
        "            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
        "        self.loss = loss\n",
        "        self.metric_ftns = [eval(metric) for metric in [\"accuracy\"]]\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Trainer configurations\n",
        "        self.epochs = 100\n",
        "        self.save_period = 30\n",
        "        self.monitor = \"min val_loss\"\n",
        "        self.start_epoch = 1\n",
        "\n",
        "        # Monitoring and early stopping setup\n",
        "        self._setup_monitoring()\n",
        "\n",
        "        # DataLoader and related attributes\n",
        "        self.data_loader = data_loader\n",
        "        self.len_epoch = len(self.data_loader)\n",
        "        self.valid_data_loader = valid_data_loader\n",
        "        self.do_validation = self.valid_data_loader is not None\n",
        "        self.log_step = int(data_loader.batch_size) * 1\n",
        "\n",
        "        # Metrics\n",
        "        self.train_metrics = MetricTracker('loss', *[m.__name__ for m in self.metric_ftns])\n",
        "        self.valid_metrics = MetricTracker('loss', *[m.__name__ for m in self.metric_ftns])\n",
        "\n",
        "        self.curr_best = 0\n",
        "\n",
        "    def _setup_monitoring(self):\n",
        "        \"\"\"\n",
        "        Helper function to setup monitoring configuration\n",
        "        \"\"\"\n",
        "        if self.monitor == 'off':\n",
        "            self.mnt_mode = 'off'\n",
        "            self.mnt_best = 0\n",
        "        else:\n",
        "            self.mnt_mode, self.mnt_metric = self.monitor.split()\n",
        "            assert self.mnt_mode in ['min', 'max']\n",
        "            self.mnt_best = inf if self.mnt_mode == 'min' else -inf\n",
        "            self.early_stop = inf\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Training loops\n",
        "        \"\"\"\n",
        "        not_improved_count = 0\n",
        "        all_outs = []\n",
        "        all_trgs = []\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
        "            result, epoch_outs, epoch_trgs = self._train_epoch(epoch, self.epochs)\n",
        "\n",
        "            # Save logged informations into log dict\n",
        "            log = {'epoch': epoch}\n",
        "            log.update(result)\n",
        "            all_outs.extend(epoch_outs)\n",
        "            all_trgs.extend(epoch_trgs)\n",
        "            # Print logged information to the screen\n",
        "            for key, value in log.items():\n",
        "                # 15s is the width of the column\n",
        "                # key is the name of the metric\n",
        "                print('    {:15s}: {}'.format(str(key), value))\n",
        "\n",
        "            # Evaluate model performance according to configured metric, save_best checkpoint as model_best\n",
        "            best = False #True\n",
        "            if self.mnt_mode != 'off':\n",
        "                try:\n",
        "                    # Check whether model performance improved or not, according to specified metric(mnt_metric)\n",
        "                    improved = (self.mnt_mode == 'min' and log[self.mnt_metric] <= self.mnt_best) or \\\n",
        "                               (self.mnt_mode == 'max' and log[self.mnt_metric] >= self.mnt_best)\n",
        "                except KeyError:\n",
        "                    print(\"Warning: Metric '{}' is not found. Model performance monitoring is disabled.\".format(self.mnt_metric))\n",
        "                    self.mnt_mode = 'off'\n",
        "                    improved = False\n",
        "\n",
        "                if improved:\n",
        "                    self.mnt_best = log[self.mnt_metric]\n",
        "                    not_improved_count = 0\n",
        "                    best = True\n",
        "                else:\n",
        "                    not_improved_count += 1\n",
        "\n",
        "                if not_improved_count > self.early_stop:\n",
        "                    print(\"Validation performance didn\\'t improve for {} epochs. Training stops.\".format(self.early_stop))\n",
        "                    break\n",
        "\n",
        "            if epoch % self.save_period == 0:\n",
        "                _save_checkpoint(self.model, self.optimizer, epoch, self.mnt_best,\n",
        "                                 self.checkpoint_dir,self.fold_id, save_best = True)\n",
        "\n",
        "        outs_name = \"outs_\" + str(self.fold_id)\n",
        "        trgs_name = \"trgs_\" + str(self.fold_id)\n",
        "        np.save(f'/content/saved/{outs_name}', all_outs)\n",
        "        np.save(f'/content/saved/{trgs_name}', all_trgs)\n",
        "\n",
        "        # Save the metrics for the last fold\n",
        "        _calc_metrics('/content/saved',self.exp_name, fold_id=self.fold_id)\n",
        "\n",
        "        # Save the metrics for the entire training\n",
        "        if self.fold_id == 87 - 1:\n",
        "            _calc_metrics(self.checkpoint_dir,self.exp_name)\n",
        "\n",
        "    def _train_epoch(self, epoch, total_epochs):\n",
        "        \"\"\"\n",
        "        Training logic for an epoch\n",
        "\n",
        "        :param epoch: Integer, current training epoch.\n",
        "               total_epochs: Integer, the total number of epoch\n",
        "        :return: A log that contains average loss and metric in this epoch.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        self.train_metrics.reset()\n",
        "        overall_outs = []\n",
        "        overall_trgs = []\n",
        "        for batch_idx, (data, target) in enumerate(self.data_loader):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "\n",
        "            loss = self.loss(output, target)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.train_metrics.update('loss', loss.item())\n",
        "            for met in self.metric_ftns:\n",
        "                self.train_metrics.update(met.__name__, met(output, target))\n",
        "\n",
        "            # When log_step == batch_size, log only shows for one entire batch\n",
        "            # print('Train Epoch: {} {} Loss: {:.6f} '.format(epoch,self._progress(batch_idx),loss.item(),))\n",
        "            if batch_idx % self.log_step == 0:\n",
        "              print('Train Epoch: {}'.format(epoch,))\n",
        "\n",
        "            if batch_idx == self.len_epoch:\n",
        "                break\n",
        "\n",
        "        log = self.train_metrics.result()\n",
        "\n",
        "        if self.do_validation:\n",
        "            val_log, outs, trgs = self._valid_epoch(epoch)\n",
        "            log.update(**{'val_' + k: v for k, v in val_log.items()})\n",
        "            if val_log[\"accuracy\"] > self.curr_best:\n",
        "                self.curr_best = val_log[\"accuracy\"]\n",
        "                selected_d[\"outs\"] = outs\n",
        "                selected_d[\"trg\"] = trgs\n",
        "            if epoch == total_epochs:\n",
        "                overall_outs.extend(selected_d[\"outs\"])\n",
        "                overall_trgs.extend(selected_d[\"trg\"])\n",
        "\n",
        "            # THIS part is to reduce the learning rate after 10 epochs to 1e-4\n",
        "            if epoch == 10:\n",
        "                for g in self.optimizer.param_groups:\n",
        "                    g['lr'] = 1e-4\n",
        "\n",
        "        return log, overall_outs, overall_trgs\n",
        "\n",
        "    def _valid_epoch(self, epoch):\n",
        "        \"\"\"\n",
        "        One validation epoch after each training epoch\n",
        "\n",
        "        :param epoch: Integer, current training epoch.\n",
        "        :return: A log that contains information about validation\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        self.valid_metrics.reset()\n",
        "        with torch.no_grad():\n",
        "            outs = np.array([])\n",
        "            trgs = np.array([])\n",
        "            for batch_idx, (data, target) in enumerate(self.valid_data_loader):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.loss(output, target)\n",
        "\n",
        "                self.valid_metrics.update('loss', loss.item())\n",
        "                for met in self.metric_ftns:\n",
        "                    self.valid_metrics.update(met.__name__, met(output, target))\n",
        "\n",
        "                preds_ = output.data.max(1, keepdim=True)[1].cpu()\n",
        "\n",
        "                outs = np.append(outs, preds_.cpu().numpy())\n",
        "                trgs = np.append(trgs, target.data.cpu().numpy())\n",
        "        return self.valid_metrics.result(), outs, trgs\n",
        "\n",
        "\n",
        "    def _progress(self, batch_idx):\n",
        "        base = '[{}/{} ({:.0f}%)]'\n",
        "        if hasattr(self.data_loader, 'n_samples'):\n",
        "            current = batch_idx * self.data_loader.batch_size\n",
        "            total = self.data_loader.n_samples\n",
        "        else:\n",
        "            current = batch_idx\n",
        "            total = self.len_epoch\n",
        "        return base.format(current, total, 100.0 * current / total)"
      ],
      "metadata": {
        "id": "Vhjhgsv3Nena"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3XKSfp2hTnRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "module_module_mscn.py\n",
        "\n",
        "This module contains the implementation of the Multiscale Convolutional Network (MSCN) architecture.\n",
        "It provides the MSCN class.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MSCN(nn.Module):\n",
        "    def __init__(self, modality):\n",
        "        super(MSCN, self).__init__()\n",
        "        dropout = 0.5\n",
        "        self.modality = modality\n",
        "\n",
        "        self.short_scale = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=50, stride=6, dilation=1, bias=False, padding=24),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool1d(kernel_size=8, stride=2, padding=4),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=8, stride=1, bias=False, padding=4),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Conv1d(128, 64, kernel_size=8, stride=1, bias=False, padding=4),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4, padding=2)\n",
        "        )\n",
        "\n",
        "        self.medium_scale = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=512, stride=42, dilation=1, bias=False, padding=256),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4, padding=0),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=4, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Conv1d(128, 64, kernel_size=4, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.long_scale = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=1024, stride=84, dilation=1, bias=False, padding=512),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool1d(kernel_size=8, stride=8, padding=0),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Conv1d(128, 64, kernel_size=7, stride=1, bias=False, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if self.modality == 1:\n",
        "          self.fc_short = nn.Linear(60,75)\n",
        "          self.fc_medium = nn.Linear(12,75)\n",
        "          self.fc_long = nn.Linear(3,75)\n",
        "        elif self.modality == 2:\n",
        "          self.fc_short = nn.Linear(119,75)\n",
        "          self.fc_medium = nn.Linear(20,75)\n",
        "          self.fc_long = nn.Linear(5,75)\n",
        "        elif self.modality == 3:\n",
        "          self.fc_short = nn.Linear(177,75)\n",
        "          self.fc_medium = nn.Linear(29,75)\n",
        "          self.fc_long = nn.Linear(7,75)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_short = self.short_scale(x)\n",
        "        # print(x_short.shape[2])\n",
        "        x_medium = self.medium_scale(x)\n",
        "        # print(x_medium.shape[2])\n",
        "        x_long = self.long_scale(x)\n",
        "        # print(x_long.shape[2])\n",
        "\n",
        "        x_short = self.fc_short(x_short)\n",
        "        x_medium = self.fc_medium(x_medium)\n",
        "        x_long = self.fc_long(x_long)\n",
        "\n",
        "        x_concat = torch.cat((x_short, x_medium, x_long), dim=1)\n",
        "\n",
        "        x_concat = self.dropout(x_concat)\n",
        "\n",
        "        return x_concat"
      ],
      "metadata": {
        "id": "JhsZPKveOYq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "module_se_resnet.py\n",
        "\n",
        "This module contains the implementation of the SEResNet architecture.\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SENet(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation block for channel-wise attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SENet, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic building block for squeeze-and-excitation networks with other layers.\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None,\n",
        "                 *, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, output_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(output_channels, output_channels, 1)\n",
        "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
        "        self.se = SENet(output_channels, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        # First convolutional layer\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        # Downsample if necessary\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # Add residual connection and apply ReLU\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual network with squeeze-and-excitation blocks\n",
        "\n",
        "    Downsampling is performed by conv1 when stride != 1 or\n",
        "    the input_channels size is not equal to the output size.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_channels, block_size):\n",
        "        super(SEResNet, self).__init__()\n",
        "        self.input_channels = 192\n",
        "        self.block = SEBasicBlock\n",
        "        self.layer = self._make_layer(self.block, output_channels, block_size)\n",
        "\n",
        "    # Create a layer with 'blocks' number of SEBasicBlock instances\n",
        "    def _make_layer(self, block, output_channels, blocks, stride = 1):\n",
        "        downsample = self._downsample_layer(self.input_channels, output_channels * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.input_channels, output_channels, stride, downsample)]\n",
        "        self.input_channels = output_channels * block.expansion\n",
        "        layers.extend(block(self.input_channels, output_channels) for _ in range(1, blocks))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    @staticmethod\n",
        "    def _downsample_layer(input_channels, output_channels, stride):\n",
        "        if stride != 1 or input_channels != output_channels:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv1d(input_channels, output_channels,\n",
        "                          kernel_size = 1, stride = stride, bias = False),\n",
        "                nn.BatchNorm1d(output_channels)\n",
        "            )\n",
        "        return None\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)"
      ],
      "metadata": {
        "id": "sZR5xOnKObu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "module_transformer_encoder.py\n",
        "\n",
        "This module contains the implementation of the Transformer Encoder architecture.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "# Utility function\n",
        "def clones(module, N):\n",
        "    \"\"\"\n",
        "    Generate N identical layers\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): PyTorch module to be cloned\n",
        "        N (int): Number of clones to create\n",
        "\n",
        "    Returns:\n",
        "        nn.ModuleList: List of N cloned PyTorch modules\n",
        "    \"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Temporal Convolutional Network with causal padding, residual connections, and batch normalization\n",
        "\n",
        "    When kernel_size equals to zero, the padding is not causal\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, causal=True):\n",
        "        super(TCN, self).__init__()\n",
        "        padding = (kernel_size - 1) * dilation if causal else 0\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding, dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.causal = causal\n",
        "        self.tcn_padding = padding\n",
        "        self.downsample = None\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        if self.causal:\n",
        "            out = out[:, :, :-self.tcn_padding]\n",
        "\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.causal:\n",
        "            out = out[:, :, :-self.tcn_padding]\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, model_dim, se_reduced_size, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.tcn = clones(TCN(se_reduced_size, se_reduced_size, kernel_size=7), 2)\n",
        "        # If set batch_first=True, for nn.MultiheadAttention()\n",
        "        # then the input and output tensors are provided as (batch, seq, feature(channels))\n",
        "        self.multihead_attention = nn.MultiheadAttention(se_reduced_size, num_heads, batch_first=True,\n",
        "                                                         dropout=dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        \"\"\"\n",
        "        We swap the seq and channel dimensions for the input tensors to\n",
        "        meet the requirements of nn.MultiheadAttention()\n",
        "        e.g. BioVid, (batch_size=128, seq_len=75, out_channels=30)\n",
        "        :return: dimension (batch_size, out_channels, seq_len)\n",
        "        \"\"\"\n",
        "        query = query.transpose(1, 2)\n",
        "        key = self.tcn[0](key).transpose(1, 2)\n",
        "        value = self.tcn[1](value).transpose(1, 2)\n",
        "        attn_output, attn_output_weights = self.multihead_attention(query, key, value)\n",
        "        # Swap back to the original dimensions\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "        return attn_output\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dim, d_mlp, dropout=0.1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.w_1 = nn.Linear(model_dim, d_mlp)\n",
        "        self.w_2 = nn.Linear(d_mlp, model_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        MLP forward pass\n",
        "        \"\"\"\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer Normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        print(self.a_2.shape, self.b_2.shape)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerOutput(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual connection followed by a layer norm.\n",
        "    \"\"\"\n",
        "    def __init__(self, se_reduced_size, dropout):\n",
        "        super(SublayerOutput, self).__init__()\n",
        "        self.norm = nn.LayerNorm(se_reduced_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        Apply residual connection to any sublayer with the same size.\n",
        "        \"\"\"\n",
        "        normalized_x = self.norm(x.transpose(1, 2)).transpose(1, 2)\n",
        "        return x + self.dropout(sublayer(normalized_x))\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder\n",
        "\n",
        "    Integration of MHA and MLP.\n",
        "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_dim, self_attn, feed_forward, se_reduced_size, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer_output = clones(SublayerOutput(se_reduced_size, dropout), 2)\n",
        "        self.size = model_dim\n",
        "        self.conv = TCN(se_reduced_size, se_reduced_size, kernel_size=7)\n",
        "\n",
        "    def forward(self, x_in):\n",
        "        query = self.conv(x_in)\n",
        "        # Encoder self-attention\n",
        "        x = self.sublayer_output[0](query, lambda x: self.self_attn(query, x_in, x_in))\n",
        "        return self.sublayer_output[1](x, self.feed_forward)\n",
        "\n",
        "\n",
        "class EncoderWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Wrapper\n",
        "\n",
        "    It is a stack of N layers of transformer encoder, default N=2.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, model_dim, se_reduced_size, d_mlp, dropout, N):\n",
        "        super(EncoderWrapper, self).__init__()\n",
        "        attn = MultiHeadAttention(num_heads, model_dim, se_reduced_size)\n",
        "\n",
        "        mlp = MLP(model_dim, d_mlp, dropout)\n",
        "        layer = TransformerEncoder(model_dim, deepcopy(attn), deepcopy(mlp), se_reduced_size, dropout)\n",
        "\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = nn.LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "sq4DXO5OOpX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PainAttnNet with Ablation Support\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PainAttnNet(nn.Module):\n",
        "    \"\"\"\n",
        "    PainAttnNet model with Ablation Support\n",
        "    \"\"\"\n",
        "    def __init__(self, modality, ablation_mode = 'full'):\n",
        "        super(PainAttnNet, self).__init__()\n",
        "        self.ablation_mode = ablation_mode\n",
        "        num_classes = 2\n",
        "\n",
        "        if self.ablation_mode == 'full':\n",
        "          N = 2 # Number of Transformer Encoder Stacks\n",
        "          model_dim = 75 # Model dimension from MSCN\n",
        "          d_mlp = 120 # Dimension of MLP\n",
        "          num_heads = 5 # Number of attention heads\n",
        "          dropout = 0.1\n",
        "          senet_reduced_size = 30 # Output SEResNet size\n",
        "\n",
        "          self.mscn = MSCN(modality) # Multiscale Convolutional Network\n",
        "          self.seresnet = SEResNet(senet_reduced_size, 1) # SEResNet\n",
        "          self.encoderWrapper = EncoderWrapper(num_heads, model_dim, senet_reduced_size, d_mlp, dropout, N) # Transformer Encoder\n",
        "          self.fc = nn.Linear(model_dim * senet_reduced_size, num_classes) # Fully connected layer to output the final prediction\n",
        "        elif self.ablation_mode == 'mscn':\n",
        "          model_dim = 75 # Model dimension from MSCN\n",
        "\n",
        "          self.mscn = MSCN(modality) # Multiscale Convolutional Network\n",
        "          self.fc = nn.Linear(192 * model_dim, num_classes) # Fully connected layer to output the final prediction\n",
        "        elif self.ablation_mode == 'mscn_se':\n",
        "          model_dim = 75 # Model dimension from MSCN\n",
        "          senet_reduced_size = 30 # Output SEResNet size\n",
        "\n",
        "          self.mscn = MSCN(modality) # Multiscale Convolutional Network\n",
        "          self.seresnet = SEResNet(senet_reduced_size, 1) # SEResNet\n",
        "          self.fc = nn.Linear(model_dim * senet_reduced_size, num_classes) # Fully connected layer to output the final prediction\n",
        "        elif self.ablation_mode == 'mscn_tran':\n",
        "          model_dim = 75 # Model dimension from MSCN\n",
        "          N = 2 # Number of Transformer Encoder Stacks\n",
        "          d_mlp = 120 # Dimension of MLP\n",
        "          num_heads = 6 # Number of attention heads\n",
        "          dropout = 0.1\n",
        "\n",
        "          self.mscn = MSCN(modality) # Multiscale Convolutional Network\n",
        "          self.encoderWrapper = EncoderWrapper(num_heads, model_dim, 192, d_mlp, dropout, N) # Transformer Encoder\n",
        "          self.fc = nn.Linear(model_dim * 192, num_classes) # Fully connected layer to output the final prediction\n",
        "\n",
        "    def forward(self, x):\n",
        "      if self.ablation_mode == 'full':\n",
        "        mscn_feat = self.mscn(x)\n",
        "        se_feat = self.seresnet(mscn_feat)\n",
        "        transformer_feat = self.encoderWrapper(se_feat)\n",
        "        # Flatten the output of Transformer Encoder to feed into the fully connected layer\n",
        "        transformer_feat = transformer_feat.contiguous().view(transformer_feat.shape[0], -1)\n",
        "        final_output = self.fc(transformer_feat)\n",
        "        return final_output\n",
        "      elif self.ablation_mode == 'mscn':\n",
        "        mscn_feat = self.mscn(x)\n",
        "        mscn_feat = mscn_feat.contiguous().view(mscn_feat.shape[0], -1)\n",
        "        final_output = self.fc(mscn_feat)\n",
        "        return final_output\n",
        "      elif self.ablation_mode == 'mscn_se':\n",
        "        mscn_feat = self.mscn(x)\n",
        "        se_feat = self.seresnet(mscn_feat)\n",
        "        se_feat = se_feat.contiguous().view(se_feat.shape[0], -1)\n",
        "        final_output = self.fc(se_feat)\n",
        "        return final_output\n",
        "      elif self.ablation_mode == 'mscn_tran':\n",
        "        mscn_feat = self.mscn(x)\n",
        "        transformer_feat = self.encoderWrapper(mscn_feat)\n",
        "        transformer_feat = transformer_feat.contiguous().view(transformer_feat.shape[0], -1)\n",
        "        final_output = self.fc(transformer_feat)\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "Mne4PV6QZZoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Multimodal PainAttnNet\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PainAttnNet_Multimodal(nn.Module):\n",
        "    \"\"\"\n",
        "    Multimodal PainAttnNet model\n",
        "    \"\"\"\n",
        "    def __init__(self, modality, fusion_type):\n",
        "        super(PainAttnNet_Multimodal, self).__init__()\n",
        "        self.modality = modality\n",
        "        self.fusion_type = fusion_type\n",
        "\n",
        "        if self.fusion_type == 'early':\n",
        "          self.PainAttnNet = PainAttnNet(modality)\n",
        "        elif self.fusion_type == 'late':\n",
        "          if modality <= 1:\n",
        "            raise ValueError(\"For late fusion, modality must be greater than 1.\")\n",
        "          model_dim = 75 # Model dimension from MSCN\n",
        "          num_classes = 2\n",
        "          senet_reduced_size = 30 # Output SEResNet size\n",
        "          # Create multiple instances of Unimodal PainAttnNet\n",
        "          self.model_list = nn.ModuleList([PainAttnNet(1) for _ in range(modality)])\n",
        "          # Learnable parameters for linear combination\n",
        "          self.weights = nn.Parameter(torch.ones(modality))\n",
        "          self.fc = nn.Linear(model_dim * senet_reduced_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.fusion_type == 'early':\n",
        "            # Return the output of the PainAttnNet with appropriate modality\n",
        "            return self.PainAttnNet(x)\n",
        "        elif self.fusion_type == 'late':\n",
        "            # Split the input tensor x into chunks along the last dimension\n",
        "            x_chunks = torch.chunk(x, self.modality, dim = -1)\n",
        "            # Forward pass through each PainAttnNet model and combine transformer outputs\n",
        "            transformer_outputs = [model.encoderWrapper(model.seresnet(model.mscn(x))) for model,x in zip(self.model_list, x_chunks)]\n",
        "            # Apply linear combination\n",
        "            combined_output = sum(weight * output for weight, output in zip(self.weights, transformer_outputs))\n",
        "            # Flatten the combined output\n",
        "            combined_output = combined_output.contiguous().view(combined_output.size(0), -1)\n",
        "            # Feed the combined output to the final fully connected layer\n",
        "            final_output = self.fc(combined_output)\n",
        "            return final_output"
      ],
      "metadata": {
        "id": "1Sd9wepHk4Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Parameters"
      ],
      "metadata": {
        "id": "b6lwpF8OzMyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ablation_mode in ['full','mscn','mscn_se','mscn_tran']:\n",
        "  for modality in [1,2,3]:\n",
        "    model = PainAttnNet(modality,ablation_mode)\n",
        "    print(f'{ablation_mode} {modality} - {sum(p.numel() for p in model.parameters() if p.requires_grad)}') # Trainable parameters\n",
        "    #print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjf4BBcSepQG",
        "outputId": "5c2d888b-583f-4f18-9f1f-b1a607076141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full 1 - 558308\n",
            "full 2 - 563483\n",
            "full 3 - 568658\n",
            "mscn 1 - 448988\n",
            "mscn 2 - 454163\n",
            "mscn 3 - 459338\n",
            "mscn_se 1 - 437408\n",
            "mscn_se 2 - 442583\n",
            "mscn_se 3 - 447758\n",
            "mscn_tran 1 - 3887000\n",
            "mscn_tran 2 - 3892175\n",
            "mscn_tran 3 - 3897350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fusion_type in ['early','late']:\n",
        "  for modality in [1,2,3]:\n",
        "    if fusion_type != 'late' or modality != 1:\n",
        "      model = PainAttnNet_Multimodal(modality,fusion_type)\n",
        "      print(f'{fusion_type} {modality} - {sum(p.numel() for p in model.parameters() if p.requires_grad)}') # Trainable parameters\n",
        "      #print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGtI0ZM61ZGO",
        "outputId": "319cdf70-f164-4bce-d846-0781835af86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "early 1 - 558308\n",
            "early 2 - 563483\n",
            "early 3 - 568658\n",
            "late 2 - 1121120\n",
            "late 3 - 1679429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Script"
      ],
      "metadata": {
        "id": "7vxLtyP4TqTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train_kfold_cv.py\n",
        "\n",
        "This module contains the implementation of the main training loop.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "SEED = 5012023\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    \"\"\"\n",
        "    Initial weights\n",
        "    \"\"\"\n",
        "    # torch.nn.init.xavier_normal_(m.weight.data)\n",
        "    if type(m) == nn.Conv2d:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif type(m) == nn.Conv1d:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif type(m) == nn.BatchNorm1d:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def train_kfold(exp_name,folds_data,fold_id,batch_size,model,label_converter,signal_types):\n",
        "\n",
        "    model.apply(weights_init_normal)\n",
        "\n",
        "    # Get optimizer\n",
        "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(trainable_params)\n",
        "\n",
        "    data_loader, valid_data_loader, data_count = load_data(folds_data[fold_id][0], folds_data[fold_id][1], label_converter, batch_size,signal_types)\n",
        "\n",
        "    # May set different weights for different classes here\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    trainer = Trainer(model, loss, optimizer,data_loader=data_loader,fold_id=fold_id,exp_name=exp_name,valid_data_loader=valid_data_loader)\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "8796inm2MbCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = 'saved'\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_name):\n",
        "    # If it exists, delete it\n",
        "    !rm -r {folder_name}\n",
        "\n",
        "# Create the empty folder\n",
        "os.makedirs(folder_name)"
      ],
      "metadata": {
        "id": "U6tpRMeDWJVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_data = generate_kfolds_index('/content/drive/MyDrive/BM5020/processed_bioVid_partA',87)\n",
        "batch_size = 128\n",
        "label_converter = {\"0\": 0,\"1\": -1,\"2\": -1,\"3\": -1,\"4\": 1} # Other combinations --> T0 vs T1, T0 vs T2, T0 vs T3\n",
        "signal_types = ['eda','ecg','emg']\n",
        "modality = len(signal_types)\n",
        "\n",
        "exp_type = 'a' # Change to 'f' for fusion/multimodal experiments\n",
        "\n",
        "if exp_type == 'a':\n",
        "  ablation_mode = 'mscn' # 'mscn' 'mscn_se' 'mscn_tran' 'full'\n",
        "  model = PainAttnNet(modality,ablation_mode)\n",
        "  exp_name = f\"T0_vs_T4_ablation_{ablation_mode}_mod_{modality}\"\n",
        "elif exp_type == 'f':\n",
        "  fusion_type = 'late' # 'early' 'late'\n",
        "  model = PainAttnNet_Multimodal(modality,fusion_type)\n",
        "  exp_name = f\"T0_vs_T4_fusion_{fusion_type}_mod_{modality}\"\n",
        "\n",
        "train_kfold(exp_name,folds_data,0,batch_size,model,label_converter,signal_types)\n",
        "\n",
        "# for fold_id in range(87):\n",
        "#   print(f\"================= Fold {fold_id} =================\")\n",
        "#   train_kfold(exp_name,folds_data,fold_id,batch_size,model,label_converter,signal_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn22Hv6EWRnN",
        "outputId": "ec1d497b-c63b-4b83-bc4f-8c99eaf38996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= Creating KFolds Index =================\n",
            "================= 87 folds dataset created =================\n",
            "Train Epoch: 1\n",
            "    epoch          : 1\n",
            "    loss           : 0.9017452641769692\n",
            "    accuracy       : 0.5257936507936508\n",
            "    val_loss       : 0.3802861273288727\n",
            "    val_accuracy   : 0.875\n",
            "Train Epoch: 2\n",
            "    epoch          : 2\n",
            "    loss           : 0.6887036937254446\n",
            "    accuracy       : 0.5823826058201058\n",
            "    val_loss       : 0.4570162892341614\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 3\n",
            "    epoch          : 3\n",
            "    loss           : 0.6472134987513224\n",
            "    accuracy       : 0.6253720238095238\n",
            "    val_loss       : 0.4146419167518616\n",
            "    val_accuracy   : 0.825\n",
            "Train Epoch: 4\n",
            "    epoch          : 4\n",
            "    loss           : 0.6487958696153429\n",
            "    accuracy       : 0.6044973544973545\n",
            "    val_loss       : 0.43803176283836365\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 5\n",
            "    epoch          : 5\n",
            "    loss           : 0.635691225528717\n",
            "    accuracy       : 0.626942791005291\n",
            "    val_loss       : 0.4599619507789612\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 6\n",
            "    epoch          : 6\n",
            "    loss           : 0.6207564892592253\n",
            "    accuracy       : 0.6498429232804233\n",
            "    val_loss       : 0.40309053659439087\n",
            "    val_accuracy   : 0.875\n",
            "Train Epoch: 7\n",
            "    epoch          : 7\n",
            "    loss           : 0.6368961113470571\n",
            "    accuracy       : 0.6421544312169312\n",
            "    val_loss       : 0.4401317238807678\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 8\n",
            "    epoch          : 8\n",
            "    loss           : 0.6241063453533031\n",
            "    accuracy       : 0.662243716931217\n",
            "    val_loss       : 0.39743146300315857\n",
            "    val_accuracy   : 0.85\n",
            "Train Epoch: 9\n",
            "    epoch          : 9\n",
            "    loss           : 0.6193913994012056\n",
            "    accuracy       : 0.6586888227513228\n",
            "    val_loss       : 0.46819815039634705\n",
            "    val_accuracy   : 0.675\n",
            "Train Epoch: 10\n",
            "    epoch          : 10\n",
            "    loss           : 0.5940903778429385\n",
            "    accuracy       : 0.6722883597883599\n",
            "    val_loss       : 0.41379356384277344\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 11\n",
            "    epoch          : 11\n",
            "    loss           : 0.5623857654907085\n",
            "    accuracy       : 0.7043650793650793\n",
            "    val_loss       : 0.3751688599586487\n",
            "    val_accuracy   : 0.875\n",
            "Train Epoch: 12\n",
            "    epoch          : 12\n",
            "    loss           : 0.5498876980057469\n",
            "    accuracy       : 0.7198660714285714\n",
            "    val_loss       : 0.3762827515602112\n",
            "    val_accuracy   : 0.85\n",
            "Train Epoch: 13\n",
            "    epoch          : 13\n",
            "    loss           : 0.5505008090425421\n",
            "    accuracy       : 0.71875\n",
            "    val_loss       : 0.37641441822052\n",
            "    val_accuracy   : 0.825\n",
            "Train Epoch: 14\n",
            "    epoch          : 14\n",
            "    loss           : 0.5380773687804187\n",
            "    accuracy       : 0.7268105158730158\n",
            "    val_loss       : 0.3813847601413727\n",
            "    val_accuracy   : 0.825\n",
            "Train Epoch: 15\n",
            "    epoch          : 15\n",
            "    loss           : 0.5357376116293447\n",
            "    accuracy       : 0.7204034391534392\n",
            "    val_loss       : 0.3882071375846863\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 16\n",
            "    epoch          : 16\n",
            "    loss           : 0.540678835577435\n",
            "    accuracy       : 0.7256531084656084\n",
            "    val_loss       : 0.37853798270225525\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 17\n",
            "    epoch          : 17\n",
            "    loss           : 0.5301967748889217\n",
            "    accuracy       : 0.7341683201058201\n",
            "    val_loss       : 0.3803855776786804\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 18\n",
            "    epoch          : 18\n",
            "    loss           : 0.5255900422732035\n",
            "    accuracy       : 0.730530753968254\n",
            "    val_loss       : 0.3662419319152832\n",
            "    val_accuracy   : 0.825\n",
            "Train Epoch: 19\n",
            "    epoch          : 19\n",
            "    loss           : 0.5290426170384442\n",
            "    accuracy       : 0.7322668650793651\n",
            "    val_loss       : 0.390429824590683\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 20\n",
            "    epoch          : 20\n",
            "    loss           : 0.5146125908251162\n",
            "    accuracy       : 0.7404513888888888\n",
            "    val_loss       : 0.38639673590660095\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 21\n",
            "    epoch          : 21\n",
            "    loss           : 0.513984904245094\n",
            "    accuracy       : 0.7472304894179894\n",
            "    val_loss       : 0.3955881595611572\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 22\n",
            "    epoch          : 22\n",
            "    loss           : 0.4964882256808104\n",
            "    accuracy       : 0.7581018518518519\n",
            "    val_loss       : 0.3853732645511627\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 23\n",
            "    epoch          : 23\n",
            "    loss           : 0.5124695245866422\n",
            "    accuracy       : 0.7446263227513228\n",
            "    val_loss       : 0.40935730934143066\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 24\n",
            "    epoch          : 24\n",
            "    loss           : 0.515177744406241\n",
            "    accuracy       : 0.7414434523809524\n",
            "    val_loss       : 0.3938151001930237\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 25\n",
            "    epoch          : 25\n",
            "    loss           : 0.49088097280926174\n",
            "    accuracy       : 0.7582258597883599\n",
            "    val_loss       : 0.38645675778388977\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 26\n",
            "    epoch          : 26\n",
            "    loss           : 0.4931652744611104\n",
            "    accuracy       : 0.7585978835978836\n",
            "    val_loss       : 0.4117191731929779\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 27\n",
            "    epoch          : 27\n",
            "    loss           : 0.4890546434455448\n",
            "    accuracy       : 0.7599619708994709\n",
            "    val_loss       : 0.42236343026161194\n",
            "    val_accuracy   : 0.7\n",
            "Train Epoch: 28\n",
            "    epoch          : 28\n",
            "    loss           : 0.4846254101505986\n",
            "    accuracy       : 0.7650876322751323\n",
            "    val_loss       : 0.39664870500564575\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 29\n",
            "    epoch          : 29\n",
            "    loss           : 0.4933685671400141\n",
            "    accuracy       : 0.7578125\n",
            "    val_loss       : 0.40650343894958496\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 30\n",
            "    epoch          : 30\n",
            "    loss           : 0.4876706655378695\n",
            "    accuracy       : 0.7605820105820106\n",
            "    val_loss       : 0.3903050720691681\n",
            "    val_accuracy   : 0.8\n",
            "Saving checkpoint: /content/saved/checkpoint-epoch30-fold0.pth ...\n",
            "Saving current best: model_best.pth ...\n",
            "Train Epoch: 31\n",
            "    epoch          : 31\n",
            "    loss           : 0.4805005446628288\n",
            "    accuracy       : 0.7648396164021164\n",
            "    val_loss       : 0.3995797634124756\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 32\n",
            "    epoch          : 32\n",
            "    loss           : 0.4746851457489861\n",
            "    accuracy       : 0.7722800925925926\n",
            "    val_loss       : 0.3934957981109619\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 33\n",
            "    epoch          : 33\n",
            "    loss           : 0.4803745592081988\n",
            "    accuracy       : 0.765500992063492\n",
            "    val_loss       : 0.4164440631866455\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 34\n",
            "    epoch          : 34\n",
            "    loss           : 0.49607469307051766\n",
            "    accuracy       : 0.754174933862434\n",
            "    val_loss       : 0.4021075665950775\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 35\n",
            "    epoch          : 35\n",
            "    loss           : 0.4642726238127108\n",
            "    accuracy       : 0.7823660714285714\n",
            "    val_loss       : 0.4082370400428772\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 36\n",
            "    epoch          : 36\n",
            "    loss           : 0.4679799399994038\n",
            "    accuracy       : 0.7795552248677249\n",
            "    val_loss       : 0.40456119179725647\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 37\n",
            "    epoch          : 37\n",
            "    loss           : 0.4524936422153755\n",
            "    accuracy       : 0.7857142857142857\n",
            "    val_loss       : 0.41402751207351685\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 38\n",
            "    epoch          : 38\n",
            "    loss           : 0.46030094005443434\n",
            "    accuracy       : 0.7797619047619047\n",
            "    val_loss       : 0.40339788794517517\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 39\n",
            "    epoch          : 39\n",
            "    loss           : 0.4538120384569521\n",
            "    accuracy       : 0.7822833994708994\n",
            "    val_loss       : 0.3925493359565735\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 40\n",
            "    epoch          : 40\n",
            "    loss           : 0.45797914376965276\n",
            "    accuracy       : 0.7786044973544973\n",
            "    val_loss       : 0.4110145568847656\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 41\n",
            "    epoch          : 41\n",
            "    loss           : 0.4395835189907639\n",
            "    accuracy       : 0.789145171957672\n",
            "    val_loss       : 0.39050692319869995\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 42\n",
            "    epoch          : 42\n",
            "    loss           : 0.4365259022624404\n",
            "    accuracy       : 0.7938988095238095\n",
            "    val_loss       : 0.41381925344467163\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 43\n",
            "    epoch          : 43\n",
            "    loss           : 0.4420111819549843\n",
            "    accuracy       : 0.7823247354497355\n",
            "    val_loss       : 0.4196201264858246\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 44\n",
            "    epoch          : 44\n",
            "    loss           : 0.4461295759236371\n",
            "    accuracy       : 0.7858796296296297\n",
            "    val_loss       : 0.45685750246047974\n",
            "    val_accuracy   : 0.7\n",
            "Train Epoch: 45\n",
            "    epoch          : 45\n",
            "    loss           : 0.4351635895393513\n",
            "    accuracy       : 0.7953455687830688\n",
            "    val_loss       : 0.4436659812927246\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 46\n",
            "    epoch          : 46\n",
            "    loss           : 0.4241550189477426\n",
            "    accuracy       : 0.8025793650793651\n",
            "    val_loss       : 0.4110330045223236\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 47\n",
            "    epoch          : 47\n",
            "    loss           : 0.43715870490780584\n",
            "    accuracy       : 0.7917493386243386\n",
            "    val_loss       : 0.4504793584346771\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 48\n",
            "    epoch          : 48\n",
            "    loss           : 0.42449442435193946\n",
            "    accuracy       : 0.7941468253968254\n",
            "    val_loss       : 0.42738279700279236\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 49\n",
            "    epoch          : 49\n",
            "    loss           : 0.42134864573125486\n",
            "    accuracy       : 0.8056795634920635\n",
            "    val_loss       : 0.38591229915618896\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 50\n",
            "    epoch          : 50\n",
            "    loss           : 0.4269013250315631\n",
            "    accuracy       : 0.7955935846560847\n",
            "    val_loss       : 0.39510858058929443\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 51\n",
            "    epoch          : 51\n",
            "    loss           : 0.4144147479975665\n",
            "    accuracy       : 0.8056795634920635\n",
            "    val_loss       : 0.3856567442417145\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 52\n",
            "    epoch          : 52\n",
            "    loss           : 0.41754460445156805\n",
            "    accuracy       : 0.8037367724867726\n",
            "    val_loss       : 0.4046410620212555\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 53\n",
            "    epoch          : 53\n",
            "    loss           : 0.4054031096122883\n",
            "    accuracy       : 0.8157655423280423\n",
            "    val_loss       : 0.4060576558113098\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 54\n",
            "    epoch          : 54\n",
            "    loss           : 0.38585744080720125\n",
            "    accuracy       : 0.8262648809523809\n",
            "    val_loss       : 0.4533458650112152\n",
            "    val_accuracy   : 0.7\n",
            "Train Epoch: 55\n",
            "    epoch          : 55\n",
            "    loss           : 0.4188388310096882\n",
            "    accuracy       : 0.8036127645502645\n",
            "    val_loss       : 0.41293877363204956\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 56\n",
            "    epoch          : 56\n",
            "    loss           : 0.3906894944332264\n",
            "    accuracy       : 0.824156746031746\n",
            "    val_loss       : 0.3893301486968994\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 57\n",
            "    epoch          : 57\n",
            "    loss           : 0.4001769909152278\n",
            "    accuracy       : 0.814484126984127\n",
            "    val_loss       : 0.39945489168167114\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 58\n",
            "    epoch          : 58\n",
            "    loss           : 0.3967907318362483\n",
            "    accuracy       : 0.8160548941798942\n",
            "    val_loss       : 0.36808085441589355\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 59\n",
            "    epoch          : 59\n",
            "    loss           : 0.3931979912298697\n",
            "    accuracy       : 0.8163855820105821\n",
            "    val_loss       : 0.40842199325561523\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 60\n",
            "    epoch          : 60\n",
            "    loss           : 0.4020619778721421\n",
            "    accuracy       : 0.8155175264550265\n",
            "    val_loss       : 0.39444440603256226\n",
            "    val_accuracy   : 0.775\n",
            "Saving checkpoint: /content/saved/checkpoint-epoch60-fold0.pth ...\n",
            "Saving current best: model_best.pth ...\n",
            "Train Epoch: 61\n",
            "    epoch          : 61\n",
            "    loss           : 0.3864732472984879\n",
            "    accuracy       : 0.8217592592592593\n",
            "    val_loss       : 0.4363235831260681\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 62\n",
            "    epoch          : 62\n",
            "    loss           : 0.39426496624946594\n",
            "    accuracy       : 0.8187830687830688\n",
            "    val_loss       : 0.37915515899658203\n",
            "    val_accuracy   : 0.8\n",
            "Train Epoch: 63\n",
            "    epoch          : 63\n",
            "    loss           : 0.38245840977739404\n",
            "    accuracy       : 0.8276703042328043\n",
            "    val_loss       : 0.4232099950313568\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 64\n",
            "    epoch          : 64\n",
            "    loss           : 0.37202470611642907\n",
            "    accuracy       : 0.8357721560846562\n",
            "    val_loss       : 0.4052537977695465\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 65\n",
            "    epoch          : 65\n",
            "    loss           : 0.37719370590315926\n",
            "    accuracy       : 0.8311838624338624\n",
            "    val_loss       : 0.4230363368988037\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 66\n",
            "    epoch          : 66\n",
            "    loss           : 0.3934920684055046\n",
            "    accuracy       : 0.8191137566137565\n",
            "    val_loss       : 0.4046017527580261\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 67\n",
            "    epoch          : 67\n",
            "    loss           : 0.3765493498908149\n",
            "    accuracy       : 0.8295304232804233\n",
            "    val_loss       : 0.39769086241722107\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 68\n",
            "    epoch          : 68\n",
            "    loss           : 0.361328669168331\n",
            "    accuracy       : 0.8393683862433862\n",
            "    val_loss       : 0.4365309774875641\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 69\n",
            "    epoch          : 69\n",
            "    loss           : 0.3664546653076454\n",
            "    accuracy       : 0.8348627645502645\n",
            "    val_loss       : 0.42185625433921814\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 70\n",
            "    epoch          : 70\n",
            "    loss           : 0.36957106435740433\n",
            "    accuracy       : 0.8337466931216931\n",
            "    val_loss       : 0.37727242708206177\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 71\n",
            "    epoch          : 71\n",
            "    loss           : 0.37328439399048136\n",
            "    accuracy       : 0.8342840608465608\n",
            "    val_loss       : 0.4079197943210602\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 72\n",
            "    epoch          : 72\n",
            "    loss           : 0.3657239929393486\n",
            "    accuracy       : 0.8336226851851852\n",
            "    val_loss       : 0.38988977670669556\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 73\n",
            "    epoch          : 73\n",
            "    loss           : 0.3495332642837807\n",
            "    accuracy       : 0.8431299603174602\n",
            "    val_loss       : 0.38841506838798523\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 74\n",
            "    epoch          : 74\n",
            "    loss           : 0.344346234092006\n",
            "    accuracy       : 0.8482969576719577\n",
            "    val_loss       : 0.3922463059425354\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 75\n",
            "    epoch          : 75\n",
            "    loss           : 0.350597678511231\n",
            "    accuracy       : 0.845651455026455\n",
            "    val_loss       : 0.37771764397621155\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 76\n",
            "    epoch          : 76\n",
            "    loss           : 0.35172925723923576\n",
            "    accuracy       : 0.8391617063492064\n",
            "    val_loss       : 0.3985850214958191\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 77\n",
            "    epoch          : 77\n",
            "    loss           : 0.3661212722460429\n",
            "    accuracy       : 0.8332506613756614\n",
            "    val_loss       : 0.39871808886528015\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 78\n",
            "    epoch          : 78\n",
            "    loss           : 0.32662985611844947\n",
            "    accuracy       : 0.8636739417989417\n",
            "    val_loss       : 0.3637283146381378\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 79\n",
            "    epoch          : 79\n",
            "    loss           : 0.33631242590921895\n",
            "    accuracy       : 0.8524718915343916\n",
            "    val_loss       : 0.41070157289505005\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 80\n",
            "    epoch          : 80\n",
            "    loss           : 0.34805038902494645\n",
            "    accuracy       : 0.8526372354497355\n",
            "    val_loss       : 0.37345999479293823\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 81\n",
            "    epoch          : 81\n",
            "    loss           : 0.32683028280735016\n",
            "    accuracy       : 0.8609457671957672\n",
            "    val_loss       : 0.38360151648521423\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 82\n",
            "    epoch          : 82\n",
            "    loss           : 0.353104861246215\n",
            "    accuracy       : 0.8373015873015873\n",
            "    val_loss       : 0.35497671365737915\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 83\n",
            "    epoch          : 83\n",
            "    loss           : 0.344349628245389\n",
            "    accuracy       : 0.8529265873015873\n",
            "    val_loss       : 0.35992756485939026\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 84\n",
            "    epoch          : 84\n",
            "    loss           : 0.3344982542373516\n",
            "    accuracy       : 0.8586309523809524\n",
            "    val_loss       : 0.3969508111476898\n",
            "    val_accuracy   : 0.725\n",
            "Train Epoch: 85\n",
            "    epoch          : 85\n",
            "    loss           : 0.34329982488243665\n",
            "    accuracy       : 0.8467675264550265\n",
            "    val_loss       : 0.37951239943504333\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 86\n",
            "    epoch          : 86\n",
            "    loss           : 0.3294039081644129\n",
            "    accuracy       : 0.8582175925925926\n",
            "    val_loss       : 0.4020184576511383\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 87\n",
            "    epoch          : 87\n",
            "    loss           : 0.33361787211011956\n",
            "    accuracy       : 0.8511904761904762\n",
            "    val_loss       : 0.42802056670188904\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 88\n",
            "    epoch          : 88\n",
            "    loss           : 0.3196101873009293\n",
            "    accuracy       : 0.8642526455026455\n",
            "    val_loss       : 0.3873569071292877\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 89\n",
            "    epoch          : 89\n",
            "    loss           : 0.31398850293071184\n",
            "    accuracy       : 0.865120701058201\n",
            "    val_loss       : 0.3600345849990845\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 90\n",
            "    epoch          : 90\n",
            "    loss           : 0.31400082177586025\n",
            "    accuracy       : 0.8609457671957672\n",
            "    val_loss       : 0.43148985505104065\n",
            "    val_accuracy   : 0.775\n",
            "Saving checkpoint: /content/saved/checkpoint-epoch90-fold0.pth ...\n",
            "Saving current best: model_best.pth ...\n",
            "Train Epoch: 91\n",
            "    epoch          : 91\n",
            "    loss           : 0.3213885465153941\n",
            "    accuracy       : 0.8602017195767195\n",
            "    val_loss       : 0.43533191084861755\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 92\n",
            "    epoch          : 92\n",
            "    loss           : 0.3093009701481572\n",
            "    accuracy       : 0.8644179894179894\n",
            "    val_loss       : 0.37292569875717163\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 93\n",
            "    epoch          : 93\n",
            "    loss           : 0.3188666977264263\n",
            "    accuracy       : 0.8606150793650793\n",
            "    val_loss       : 0.4118255078792572\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 94\n",
            "    epoch          : 94\n",
            "    loss           : 0.30725142028596664\n",
            "    accuracy       : 0.8654927248677249\n",
            "    val_loss       : 0.3797610104084015\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 95\n",
            "    epoch          : 95\n",
            "    loss           : 0.3069496966070599\n",
            "    accuracy       : 0.8680968915343916\n",
            "    val_loss       : 0.3801816701889038\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 96\n",
            "    epoch          : 96\n",
            "    loss           : 0.31221813515380575\n",
            "    accuracy       : 0.8617724867724867\n",
            "    val_loss       : 0.3806320130825043\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 97\n",
            "    epoch          : 97\n",
            "    loss           : 0.3067191011375851\n",
            "    accuracy       : 0.8619378306878306\n",
            "    val_loss       : 0.38601770997047424\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 98\n",
            "    epoch          : 98\n",
            "    loss           : 0.2967066372986193\n",
            "    accuracy       : 0.871031746031746\n",
            "    val_loss       : 0.34828999638557434\n",
            "    val_accuracy   : 0.775\n",
            "Train Epoch: 99\n",
            "    epoch          : 99\n",
            "    loss           : 0.3085887708045818\n",
            "    accuracy       : 0.8662781084656084\n",
            "    val_loss       : 0.3675307631492615\n",
            "    val_accuracy   : 0.75\n",
            "Train Epoch: 100\n",
            "    epoch          : 100\n",
            "    loss           : 0.30548311880341283\n",
            "    accuracy       : 0.8676008597883599\n",
            "    val_loss       : 0.33768123388290405\n",
            "    val_accuracy   : 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename('saved',exp_name)"
      ],
      "metadata": {
        "id": "Sdw7qHPmtAse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_folder_path = f'/content/{exp_name}'\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = new_folder_path + '.zip'\n",
        "\n",
        "if os.path.exists(zip_file_path):\n",
        "    os.remove(zip_file_path)\n",
        "\n",
        "# Compress the folder into a zip file\n",
        "!zip -r {zip_file_path} {os.path.basename(new_folder_path)}"
      ],
      "metadata": {
        "id": "_g2-iTUEzf-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15dd53de-be3b-4f92-d729-2199d783ee38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: T0_vs_T4_ablation_mscn_mod_3/ (stored 0%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/model_best_fold0.pth (deflated 8%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/trgs_0.npy (deflated 83%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/checkpoint-epoch30-fold0.pth (deflated 8%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/T0_vs_T4_ablation_mscn_mod_3_confusion_matrix_fold0.torch (deflated 62%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/outs_0.npy (deflated 80%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/checkpoint-epoch90-fold0.pth (deflated 8%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/T0_vs_T4_ablation_mscn_mod_3_classification_report_fold0.xlsx (deflated 11%)\n",
            "  adding: T0_vs_T4_ablation_mscn_mod_3/checkpoint-epoch60-fold0.pth (deflated 8%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(f'{exp_name}.zip')\n",
        "files.download(f'{exp_name}_classification_report_all_fold.xlsx')\n",
        "files.download(f'{exp_name}_confusion_matrix_all_fold.torch')"
      ],
      "metadata": {
        "id": "dZlX-ro9y42J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b528e39c-2828-482f-e7e9-60a2cca1c2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_16899af3-c66f-4d65-a631-93cda980d451\", \"T0_vs_T4_ablation_mscn_mod_3.zip\", 20434847)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n"
      ],
      "metadata": {
        "id": "347xnHWnCDR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [PainAttnNet GitHub Code](https://github.com/zhenyuanlu/PainAttnNet)"
      ],
      "metadata": {
        "id": "E5Luqh5eCWSK"
      }
    }
  ]
}